{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7R2j7l8RCwx4"
   },
   "source": [
    "<img src=\"https://drive.google.com/uc?id=181S0KcAdGeAajZ1apcbOvoh3wvYuKtzd\">\n",
    "<b>Autores</b>:\n",
    "\n",
    "*   Aldo von Wangenheim (aldo.vw@ufsc.br)\n",
    "*   Rodrigo de Paula e Silva Ribeiro (ribeiro.rodrigo@posgrad.ufsc.br)\n",
    "\n",
    "\n",
    "CERTIFIQUE-SE DE TER INCIADO O AMBIENTE EM MODO <b>GPU</b> ANTES DE REALIZAR OS PROCEDIMENTOS.<br>\n",
    "NA TROCA DO AMBIENTE ELE <b><u>PERDE</u></b> TODOS OS DADOS BAIXADOS.\n",
    "\n",
    "AO RE-EXECUTAR OS PROCEDIMENTOS (ex. diminuir o batch size), <br>REINICIE O AMBIENTE PARA LIBERAR O CACHE DO PYTORCH (reiniciar não perde os dados da sessão).<br>(alt+m ou pelo menu \"Ambiente de Execução\")\n",
    "\n",
    "EXECUTE UTILIZANDO RUN ALL ou CTRL+F9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/awangenh/vision/blob/master/jupyter/14.3.Detecção%20de%20Sinais%20de%20Trânsito%20-%20YOLOv9.ipynb\"><img align=\"left\"  src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\" title=\"Open and Execute in Google Colaboratory\"></a>&nbsp; &nbsp;<a href=\"\"><img align=\"left\" src=\"http://www.lapix.ufsc.br/wp-content/uploads/2019/04/License-CC-BY-ND-4.0-orange.png\" alt=\"Creative Commons 4.0 License\" title=\"Creative Commons 4.0 License\"></a>&nbsp; &nbsp; <a href=\"\"><img align=\"left\" src=\"http://www.lapix.ufsc.br/wp-content/uploads/2019/04/Jupyter-Notebook-v.1.0-blue.png\" alt=\"Jupyter Version\" title=\"Jupyter Version\"></a>&nbsp; &nbsp;<a href=\"\"><img align=\"left\"  src=\"https://img.shields.io/badge/python-3.10-greeng\" alt=\"Python Version\" title=\"Python Version\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i9BV-PR-W1u2",
    "outputId": "c110e2c5-4cc7-4485-d7b2-ecbb18af825f"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configurações de GPU e PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc --version\n",
    "\n",
    "try:\n",
    "    import google.colab\n",
    "    _ON_COLAB = True\n",
    "except:\n",
    "    _ON_COLAB = False\n",
    "    %reload_ext autoreload\n",
    "    %autoreload 2\n",
    "    %matplotlib inline\n",
    "    from IPython import get_ipython\n",
    "    \n",
    "print('Running on Google Colab = ', _ON_COLAB)\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# Definir o caminho base\n",
    "if _ON_COLAB:\n",
    "    BASE_PATH = \"/content\"\n",
    "    import locale\n",
    "    locale.getpreferredencoding = lambda: \"UTF-8\"\n",
    "else:\n",
    "    BASE_PATH = \"/lapix\" # trocar para o base path do seu computador se estiver rodando localmente, deixar /lapix se estiver rodando nos conteiners lapix\n",
    "    VISIBLE_GPUS = [7] # Selecionar a GPU PARA RODAR! VER QUAL ESTA LIVRE\n",
    "\n",
    "    if torch.cuda.device_count() != 8:\n",
    "        print(\"GPU SETADA - PULANDO ETAPA\")\n",
    "    else:\n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(VISIBLE_GPUS).replace(\"]\", \"\").replace(\"[\",\"\").replace(\" \", \"\")\n",
    "        os.environ[\"NVIDIA_VISIBLE_DEVICES\"] = str(VISIBLE_GPUS).replace(\"]\", \"\").replace(\"[\",\"\").replace(\" \", \"\")\n",
    "        print(\"CUDA GPUS NUMBER: \", torch.cuda.device_count())\n",
    "    \n",
    "os.chdir(BASE_PATH) # garantir que está executando no caminho base definido"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FlBPQTykbG2d"
   },
   "source": [
    "# Definir a pasta base atual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fDFlUtGFbGDT",
    "outputId": "35775463-7288-4d26-c763-86d70c0ab90a"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "HOME_DIR = os.getcwd()\n",
    "print(HOME_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1TllDbGRUnk6"
   },
   "source": [
    "# O que é YOLO?\n",
    "YOLO (“You Only Look Once”), é uma série de detectores de objetos de processamento rápido que vem sendo desenvolvida por diversos autores. Ao contrário dos detectores tradicionais que utilizam áreas de interesse para propor regiões (ROI) que podem conter um objeto na imagem (conhecidos como detectores de 2-estágios), a arquitetura YOLO divide a imagem em grids e vasculha a imagem como um todo (detector de um único estágio).\n",
    "\n",
    "Neste notebook didático, é utilizado a versão 9 desta arquitetura.<br>\n",
    "Algumas notas sobre a versão 9:<br>\n",
    "* Em versões prévias era apenas possível utilizar detecção baseada em bounding box, desde a versão 7 já é possível utilizar também segmentação por instância e keypoint.\n",
    "* Na versão 9, utiliza-se apenas o método OneCycle para otimização da Learning Rate, portanto não é necessário definir uma Learning Rate, a não ser que seja desejável utilizar Learning Rate fixa (linear).\n",
    "* Foi conceitualizada pelos mesmos autores da YoloV4, Scaled-YoloV4 e YoloR. De fato é uma versão baseada na Scaled-YoloV4 com muitas melhorias.\n",
    "* Diferente de outras versões que os pesos eram pré-treinados no conjunto imagenet, a versão 9, assim como a Scaled YoloV4, é pré-treinada no [COCO Dataset](https://cocodataset.org/#home).\n",
    "\n",
    "Diferentemente do Detectron2 (framework customizável), YOLO é uma arquitetura parametrizada e única, embora é possível alterar diversos parâmetros no arquivo train.py oficial, muitas alterações ou valores muito diferentes podem descaracterizar a rede perdendo a sua capacidade de atingir resultados SOTA (state of the art).\n",
    "\n",
    "Neste notebook apenas alguns parâmetros de performance são editáveis, para editar parâmetros mais específicos ou até mesmo implementar modificações, é recomendável ler e se familiarizar com a arquitetura.\n",
    "\n",
    "Links:<br>\n",
    "[Link Artigo](https://arxiv.org/abs/2402.13616) <br>\n",
    "[Link git](https://github.com/WongKinYiu/yolov9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MrLaA--UTwnj"
   },
   "source": [
    "# Instalação do YoloV9 do repositório oficial e suas dependências."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PfRGCg8rbWmp",
    "outputId": "67ce5984-d52f-43e1-d66a-de9a4fd461a9"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/WongKinYiu/yolov9.git\n",
    "os.chdir(os.path.join(HOME_DIR, \"yolov9\"))\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xOZzHrymT_Wv"
   },
   "source": [
    "# Montar o google drive\n",
    "Se estiver rodando no Colab e quiser salvar no drive, execute a celula abaixo para montar o seu google drive e defina a pasta que será salva no Bloco de Parametros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aNjMVBzfT30G",
    "outputId": "debe5eb9-4ebf-4453-a090-dba3502d9917"
   },
   "outputs": [],
   "source": [
    "if _ON_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount(os.path.join(HOME_DIR, 'gdrive'), force_remount=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fM6ZuDTLV-Xh"
   },
   "source": [
    "# Baixar o dataset de itens de trânsito já no formato YOLO\n",
    "O dataset de itens de trânsito tem como objetivo didático demonstrar a possibilidade de identificar alguns itens comuns de trânsito.<br>\n",
    "É um conjunto anotado no formato \"bounding box\" para detecção de objetos.<br>\n",
    "O conjunto possui 4 classes distintas:\n",
    "* trafficlight\n",
    "* stop\n",
    "* speedlimit\n",
    "* crosswalk\n",
    "\n",
    "Amostra de anotações:<br>\n",
    "<img src=\"https://drive.google.com/uc?id=1AoC87UUaJ5v3W03mTE6gQocW6sYgCEuz\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bu6VgY8uV2u4",
    "outputId": "2c267944-9194-48a0-f4cc-d4483b64dbf9"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "try:\n",
    "  import gdown\n",
    "  print(\"gdown já está instalado\")\n",
    "except ModuleNotFoundError:\n",
    "  !python -m pip install gdown\n",
    "os.chdir(HOME_DIR)\n",
    "!gdown 1sIhCtoOMnpCXlByxDV95iRQDWjTPKr9v\n",
    "!unzip -qq -u trafficsign.zip\n",
    "!rm -rf trafficsign.zip\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1LecEvYduXdc"
   },
   "source": [
    "# Baixar o peso do modelo padrão para transfer learning.\n",
    "Para mais opções de pesos pré-treinados e modelos maiores/menores, ver o github oficial do [YoloV9](https://github.com/WongKinYiu/yolov9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "52uZgsQLuSO2",
    "outputId": "2c7b2f6c-7d79-4e49-f99f-a2892d83de40"
   },
   "outputs": [],
   "source": [
    "os.chdir(os.path.join(HOME_DIR, \"yolov9\"))\n",
    "!wget https://github.com/WongKinYiu/yolov9/releases/download/v0.1/yolov9-c.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xfJYfYewjzuc"
   },
   "source": [
    "# Parâmetros básicos\n",
    "Implementação de edição dos parâmetros básicos de performance do Yolo v9.<br>\n",
    "Para parâmetros opcionais e outros, ver [este código do git (linhas 438 à 487)](https://github.com/WongKinYiu/yolov9/blob/main/train_dual.py) e adequar o código.<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1V4_MW1SXGY_"
   },
   "outputs": [],
   "source": [
    "# Definir onde salvar os resultados de treino\n",
    "#SAVE_PATH = \"{HOME_DIR}/gdrive/MyDrive/aula_yoloV9/\" # para salvar na pasta 'aula_yoloV9' do seu drive, descomente esta linha e comente a linha abaixo.\n",
    "SAVE_PATH = os.path.join(HOME_DIR, \"yolov9_run\") # salvar no ambiente local\n",
    "\n",
    "os.makedirs(SAVE_PATH, exist_ok=True)\n",
    "\n",
    "# Definir o batch size\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "# Definir o número de épocas de treino\n",
    "EPOCHS = 10\n",
    "\n",
    "# Definir o formato de imagem para rede\n",
    "IM_SIZE = 640 # padrão 640, imagem menor = mais rápido, menos preciso em objetos pequenos; imagem maior = mais lento, mais preciso em objetos pequenos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dNs2zbG9UwsX",
    "outputId": "8aec1d17-3bef-4a47-efc1-f1829b79e1fb"
   },
   "outputs": [],
   "source": [
    "os.chdir(os.path.join(HOME_DIR, \"yolov9\"))\n",
    "!python train_dual.py --device 0 --batch {BATCH_SIZE} --data \"{HOME_DIR}/trafficsign/data.yaml\" --img {IM_SIZE} --cfg models/detect/yolov9-c.yaml --weights 'yolov9-c.pt' --hyp hyp.scratch-high.yaml --min-items 0 --epochs {EPOCHS} --close-mosaic 15 --project \"{SAVE_PATH}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Pln7M0P_tvC"
   },
   "source": [
    "# Gráficos de treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Y9Q-c9sJ_VKu",
    "outputId": "cce1e872-6c1f-40c3-815e-008202062447"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "metrics_img = glob(os.path.join(SAVE_PATH, 'exp/*.png'))\n",
    "n_img = len(metrics_img)\n",
    "n_cols = 2\n",
    "n_rows = int(n_img / n_cols) + (1 if n_img % n_cols != 0 else 0)\n",
    "_, axs = plt.subplots(n_rows, n_cols, figsize=(15, 15))\n",
    "axs = axs.flatten()\n",
    "for m, ax in zip(metrics_img, axs):\n",
    "  img = plt.imread(m)\n",
    "  ax.imshow(img)\n",
    "  ax.xaxis.set_major_locator(ticker.NullLocator())\n",
    "  ax.yaxis.set_major_locator(ticker.NullLocator())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ROBTBM-0ae1W"
   },
   "source": [
    "# Realizar inferência de métricas no conjunto de validação (val/valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2SW0POpwYodO",
    "outputId": "1f1ecacf-dd23-41a7-ef5d-9ded0a68af1a"
   },
   "outputs": [],
   "source": [
    "curr_weight = os.path.join(SAVE_PATH, \"exp/weights/best.pt\") # trocar exp por exp1/exp2/exp3 conforme o número do último treino gerado. É incrementado automaticamente.\n",
    "!python val_dual.py --data \"{HOME_DIR}/trafficsign/data.yaml\" --batch {BATCH_SIZE} --iou-thres 0.5 --device 0 --weights \"{curr_weight}\" --project \"{SAVE_PATH}\" --name \"val\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zlqoOJKyggIm"
   },
   "source": [
    "# Visualização dos gráficos das métricas de validação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "61qauyC1f6M7",
    "outputId": "e7df1c0e-ee74-4ef9-a8a4-13ce1908fea7"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "metrics_img = glob(os.path.join(SAVE_PATH, \"val/*.png\"))\n",
    "n_img = len(metrics_img)\n",
    "n_cols = 2\n",
    "n_rows = int(n_img / n_cols) + (1 if n_img % n_cols != 0 else 0)\n",
    "_, axs = plt.subplots(n_rows, n_cols, figsize=(15, 15))\n",
    "axs = axs.flatten()\n",
    "for m, ax in zip(metrics_img, axs):\n",
    "  img = plt.imread(m)\n",
    "  ax.imshow(img)\n",
    "  ax.xaxis.set_major_locator(ticker.NullLocator())\n",
    "  ax.yaxis.set_major_locator(ticker.NullLocator())\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EyUKgAYQYAfX"
   },
   "source": [
    "# Visualizar detecções no conjunto de teste (test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DaOhZ8c8YF4S",
    "outputId": "fcd7a4c3-fc31-4f7f-85bb-b30dae59a3f4"
   },
   "outputs": [],
   "source": [
    "curr_weight = os.path.join(SAVE_PATH, \"exp/weights/best.pt\") # trocar exp por exp1/exp2/exp3 conforme o número do último treino gerado. É incrementado automaticamente.\n",
    "!python detect_dual.py --weights \"{curr_weight}\" --conf-thres 0.7 --source \"{HOME_DIR}/trafficsign/images/test\" --project \"{SAVE_PATH}\" --name \"detect\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "zdCa3Hltw7HZ",
    "outputId": "44863d17-57de-42b3-a100-6dc27fdf5431"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "import matplotlib.ticker as ticker\n",
    "import random\n",
    "\n",
    "num_samples = 4 # numero de imagens para visualizar.\n",
    "all_dets = glob(os.path.join(SAVE_PATH, \"detect/*.png\"))\n",
    "n_cols = 2\n",
    "n_rows = int(num_samples / n_cols) + (1 if num_samples % n_cols != 0 else 0)\n",
    "_, axs = plt.subplots(n_rows, n_cols, figsize=(15, 15))\n",
    "axs = axs.flatten()\n",
    "for m, ax in zip(random.sample(all_dets, num_samples), axs):\n",
    "  img = plt.imread(m)\n",
    "  ax.imshow(img)\n",
    "  ax.xaxis.set_major_locator(ticker.NullLocator())\n",
    "  ax.yaxis.set_major_locator(ticker.NullLocator())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r7Ww23uSDWlB"
   },
   "source": [
    "<img src=\"http://lapix.ufsc.br/wp-content/uploads/2022/10/rodape-lapix.png\">"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
