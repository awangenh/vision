{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "07UCeAb5diRN"
   },
   "source": [
    "<img src=\"https://drive.google.com/uc?id=181S0KcAdGeAajZ1apcbOvoh3wvYuKtzd\">\n",
    "<b>Autores</b>:\n",
    "\n",
    "*   Aldo von Wangenheim (aldo.vw@ufsc.br)\n",
    "*   Rodrigo de Paula e Silva Ribeiro (ribeiro.rodrigo@posgrad.ufsc.br)\n",
    "\n",
    "\n",
    "CERTIFIQUE-SE DE TER INCIADO O AMBIENTE EM MODO <b>GPU</b> ANTES DE REALIZAR OS PROCEDIMENTOS.<br>\n",
    "NA TROCA DO AMBIENTE ELE <b><u>PERDE</u></b> TODOS OS DADOS BAIXADOS.\n",
    "\n",
    "AO RE-EXECUTAR OS PROCEDIMENTOS (ex. diminuir o batch size), <br>REINICIE O AMBIENTE PARA LIBERAR O CACHE DO PYTORCH (reiniciar não perde os dados da sessão).<br>(alt+m ou pelo menu \"Ambiente de Execução\")\n",
    "\n",
    "EXECUTE UTILIZANDO RUN ALL ou CTRL+F9\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github.com/awangenh/vision/blob/master/jupyter/14.2.DETECTRON%20v.2%20Detec%C3%A7%C3%A3o%20Cones%20-%20Formula%20SAE%20Driverless.ipynb\"><img align=\"left\"  src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\" title=\"Open and Execute in Google Colaboratory\"></a>&nbsp; &nbsp;<a href=\"\"><img align=\"left\" src=\"http://www.lapix.ufsc.br/wp-content/uploads/2019/04/License-CC-BY-ND-4.0-orange.png\" alt=\"Creative Commons 4.0 License\" title=\"Creative Commons 4.0 License\"></a>&nbsp; &nbsp; <a href=\"\"><img align=\"left\" src=\"http://www.lapix.ufsc.br/wp-content/uploads/2019/04/Jupyter-Notebook-v.1.0-blue.png\" alt=\"Jupyter Version\" title=\"Jupyter Version\"></a>&nbsp; &nbsp;<a href=\"\"><img align=\"left\"  src=\"https://img.shields.io/badge/python-3.10-greeng\" alt=\"Python Version\" title=\"Python Version\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "98DBj51-2oi6"
   },
   "source": [
    "#ESTE BLOCO ABAIXO FUNCIONA COMO UM LOG DE TODAS AS CELULAS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "executionInfo": {
     "elapsed": 258,
     "status": "ok",
     "timestamp": 1669090410501,
     "user": {
      "displayName": "Rodrigo Ribeiro",
      "userId": "10521807329412892963"
     },
     "user_tz": 180
    },
    "id": "la2g5A1F1qUC",
    "outputId": "3445d400-91d0-49b8-f0b8-eb8ae4157707"
   },
   "outputs": [],
   "source": [
    "#@title Célula de log\n",
    "%%javascript\n",
    "const listenerChannel = new BroadcastChannel('logger');\n",
    "listenerChannel.onmessage = (msg) => {\n",
    "  const div = document.createElement('div');\n",
    "  div.textContent = msg.data;\n",
    "  document.body.appendChild(div);\n",
    "};"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 270,
     "status": "ok",
     "timestamp": 1669090414154,
     "user": {
      "displayName": "Rodrigo Ribeiro",
      "userId": "10521807329412892963"
     },
     "user_tz": 180
    },
    "id": "y5g7hEfqfb50",
    "outputId": "e6726057-2890-46ff-bfc7-b77b2186b1d2"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configurações PATH e GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc --version\n",
    "\n",
    "try:\n",
    "    import google.colab\n",
    "    _ON_COLAB = True\n",
    "except:\n",
    "    _ON_COLAB = False\n",
    "    %reload_ext autoreload\n",
    "    %autoreload 2\n",
    "    %matplotlib inline\n",
    "    from IPython import get_ipython\n",
    "    \n",
    "print('Running on Google Colab = ', _ON_COLAB)\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# Definir o caminho base\n",
    "if _ON_COLAB:\n",
    "    BASE_PATH = \"/content\"\n",
    "    import locale\n",
    "    locale.getpreferredencoding = lambda: \"UTF-8\"\n",
    "else:\n",
    "    BASE_PATH = \"/lapix\" # trocar para o base path do seu computador se estiver rodando localmente, deixar /lapix se estiver rodando nos conteiners lapix\n",
    "    VISIBLE_GPUS = [7] # Selecionar a GPU PARA RODAR! VER QUAL ESTA LIVRE\n",
    "\n",
    "    if torch.cuda.device_count() != 8:\n",
    "        print(\"GPU SETADA - PULANDO ETAPA\")\n",
    "    else:\n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(VISIBLE_GPUS).replace(\"]\", \"\").replace(\"[\",\"\").replace(\" \", \"\")\n",
    "        os.environ[\"NVIDIA_VISIBLE_DEVICES\"] = str(VISIBLE_GPUS).replace(\"]\", \"\").replace(\"[\",\"\").replace(\" \", \"\")\n",
    "        print(\"CUDA GPUS NUMBER: \", torch.cuda.device_count())\n",
    "    \n",
    "os.chdir(BASE_PATH) # garantir que está executando no caminho base definido"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iP1CD-VoWJsu"
   },
   "source": [
    "# Configurações dos Loggers Externos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "akusqLtXDhtP"
   },
   "source": [
    "## Descrição dos Loggers e suas funções.\n",
    "Este notebook oferece logging de informações do treino para ambiente externo.\n",
    "Mais especificamente para: [WandB](https://wandb.ai/) e [Neptune](https://neptune.ai/).<br>\n",
    "WandB e Neptune são gerenciadores de projetos de machine learning que possibilitam a comparação analítica de maneira facilitada entre vários treinos de um mesmo modelo comparando diferentes parâmetros.<br>\n",
    "Há ainda opções avançadas para salvar modelos e datasets na nuvem (necessita assinatura paga) de forma a facilitar a portabilidade entre diferentes frameworks de ML.<br>\n",
    "Para utilizá-los, é necessário acessar o site (clikar nos links acima) e efetuar um cadastro gratuito para obter uma chave de api.<br>\n",
    "Após, alterar no bloco abaixo, os locais indicados com as chaves e mudar a variável de ativação para \"True\".<br>\n",
    "Caso não deseje utilizar loggers externos, manter as variáveis de ativação em \"False\".<br>\n",
    "O [Detectron2](https://github.com/facebookresearch/detectron2), por padrão, ao final do treino, irá gerar um arquivo 'metrics.json', onde é possível visualizar e analisar os dados de treino e eventualmente montar seu próprio comparador analítico entre treinos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D0Q7NL61TL_T"
   },
   "outputs": [],
   "source": [
    "PLOT_LOSS = True # Ativa gráfico de loss em tempo real no notebook no bloco (Training Loss)\n",
    "\n",
    "\"\"\"\n",
    "# EXTERNAL LOGGERS\n",
    "\"\"\"\n",
    "WANDB_LOGGER = False # Ativa o log das métricas básicas no wandb.ai, requer mais configurações abaixo.\n",
    "NPT_LOGGER = False # Ativa o log das métricas básicas no nepture.ai, requer mais configurações abaixo.\n",
    "\n",
    "\"\"\"\n",
    "# WANDB CONFIG\n",
    "\"\"\"\n",
    "WANDB_API_KEY = \"***\" # wandb.ai api-key, trocar *** por sua api key\n",
    "WANDB_PROJECT_NAME = \"Cones-Teste\" # Nome do projeto onde será logado as métricas (pode ser projeto já existente), caso deixe vazio será gerado um nome aleatório no wandb.ai\n",
    "WANDB_ENTITY = \"wndb_username\" # A entidade que está enviando os dados.\n",
    "WANDB_RUN_ID = \"\" # Em caso de resumo é necessário passar o RUN-ID para continuar logando as métricas na mesma run.\n",
    "\n",
    "\"\"\"\n",
    "# NEPTUNE CONFIG\n",
    "\"\"\"\n",
    "NPT_PROJECT_NAME = \"---\" # O nome do projeto para o qual será enviado os dados (definido no neptune.ai).\n",
    "NPT_TOKEN = \"***\" # A api token do neptune.ai, trocar *** pela sua api key fornecida no neptune.ai\n",
    "NPT_RUN_ID = \"\" # Em caso de resumo é necessário passar o RUN-ID para continuar logando na mesma run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 31033,
     "status": "ok",
     "timestamp": 1669090459560,
     "user": {
      "displayName": "Rodrigo Ribeiro",
      "userId": "10521807329412892963"
     },
     "user_tz": 180
    },
    "id": "TU3aDGtyWaCT",
    "outputId": "64cabe1b-bc18-4a6e-9273-e8c5bb86d544"
   },
   "outputs": [],
   "source": [
    "!pip install wandb -q\n",
    "!pip install neptune-client -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dnOKJnoosHbk"
   },
   "source": [
    "#Training Loss (tempo real)\n",
    "Visualização da curva de loss em tempo real durante o treino.<br>\n",
    "Este é um \"workaround\" específico para o colab para gerar um gráfico de loss em tempo real durante o treino. Ele é ativado pela variável \"PLOT_LOSS\" do bloco acima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "executionInfo": {
     "elapsed": 226,
     "status": "ok",
     "timestamp": 1669090466235,
     "user": {
      "displayName": "Rodrigo Ribeiro",
      "userId": "10521807329412892963"
     },
     "user_tz": 180
    },
    "id": "K8OErKzNsFkA",
    "outputId": "6e79cdb9-26e4-4dec-ede5-8b3ffb9b3c54"
   },
   "outputs": [],
   "source": [
    "#@title Gráfico Loss\n",
    "import IPython\n",
    "import numpy as np\n",
    "from IPython.display import display, HTML, Javascript\n",
    "import time\n",
    "import random\n",
    "\n",
    "def configure_browser_state():\n",
    "  \n",
    "  #display(Javascript('''google.colab.output.setIframeHeight(\"500\")'''))\n",
    "  display(IPython.core.display.HTML(''' <script src=\"https://cdn.jsdelivr.net/npm/chart.js@3.9.1\"></script>\n",
    "        <script>\n",
    "          const _div1 = document.createElement('div');\n",
    "          _div1.id = \"mdiv1\";\n",
    "          _div1.style.width = \"50%\";\n",
    "          document.body.appendChild(_div1);\n",
    "          const _canvas = document.createElement('canvas');\n",
    "          _canvas.id = \"lossChart\";\n",
    "          //_canvas.setAttribute(\"height\", \"80\"); // usar em caso de width > 50% para limitar altura\n",
    "          document.getElementById('mdiv1').appendChild(_canvas);\n",
    "          var ctx = document.getElementById('lossChart').getContext('2d');\n",
    "          var chart = new Chart(ctx, {\n",
    "              type: 'line',\n",
    "\n",
    "              data: {\n",
    "                  labels: [],\n",
    "                  datasets: [{\n",
    "                      label: 'train_loss',\n",
    "                      borderColor: 'rgb(255, 99, 132)',\n",
    "                      data: [],\n",
    "                      //tension: 0.4, // smoothing\n",
    "                      pointRadius: 0\n",
    "                  }]\n",
    "              },\n",
    "              \n",
    "              options: {\n",
    "                scales: {\n",
    "                  y: {\n",
    "                    type: 'linear', // 'logarithmic'\n",
    "                    min: -0.1,\n",
    "                    max: 1.0,\n",
    "                    ticks: {\n",
    "                      stepSize: 0.25\n",
    "                    }\n",
    "                  },\n",
    "                  x: {\n",
    "                    type: 'linear',\n",
    "                    beginAtZero: 1,\n",
    "                    ticks: {\n",
    "                      maxRotation: 50,\n",
    "                      minRotation: 0,\n",
    "                      maxTicksLimit: 10\n",
    "                    }\n",
    "                  }\n",
    "                },\n",
    "                animation: {\n",
    "                  duration: 0, // sem animação\n",
    "                }\n",
    "              }\n",
    "\n",
    "          });\n",
    "\n",
    "          \n",
    "\n",
    "          function addData(label, value){\n",
    "            chart.data.labels.push(label)\n",
    "            chart.data.datasets[0].data.push(value)\n",
    "            chart.update();\n",
    "          }\n",
    "          const listenerChannel = new BroadcastChannel('losslogger');\n",
    "          listenerChannel.onmessage = (msg) => {\n",
    "          addData(msg.data.split(\",\")[0],msg.data.split(\",\")[1]);\n",
    "          };\n",
    "        </script>\n",
    "        '''))\n",
    "if PLOT_LOSS:\n",
    "  configure_browser_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MYmRjYFSdUO-"
   },
   "source": [
    "# Cones Dataset\n",
    "O dataset de cones tem por objetivo possibilitar identificar diferentes tipos de cones sinalizadores de trânsito.<br>\n",
    "É um conjunto anotado no formato \"bounding box\" para detecção de objetos.<br>\n",
    "O conjunto possui 5 classes distintas:\n",
    "* blue_cone\n",
    "* large_orange_cone\n",
    "* orange_cone\n",
    "* yellow_cone\n",
    "* unknown_cone\n",
    "\n",
    "Amostra de anotações:<br>\n",
    "<img src=\"https://drive.google.com/uc?id=1WgjF0WaRLahBQyLRCAWScPSpIAFkgzJu\">\n",
    "<img src=\"https://drive.google.com/uc?id=1WgAlliwetRb4zBE_0AUMQvxrtjp4A4_e\">\n",
    "<img src=\"https://drive.google.com/uc?id=15t9g5d9PfRJx_Iss7qhP1vvBHPbNgQoX\">\n",
    "<img src=\"https://drive.google.com/uc?id=1KErMuro6T0_w17d6Gqfgtwv76h6CarNW\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B6pvEJIWCPyZ"
   },
   "source": [
    "#0 Definição das variáveis de controle (treino, resumo, batchsize, epoch, lr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z8fkhvOSJa72"
   },
   "source": [
    "<center>Diferenças das políticas de decaimento da Learning Rate.</center><br>\n",
    "LR_METHODS.COSINE: Nesta política, após o aquecimento (subida) a Learning Rate irá decair suavemente com tendência a 0.<br>\n",
    "LR_METHODS.STEP: Nesta política, após o aquecimento (subida) a Learning Rate irá decair por valores fixos e nos passos fixados previamente, tende a um valor específico resultado do decaimento do último passo.\n",
    "<br><br>\n",
    "<center>Exemplos de curvas das políticas de decaimento da Learning Rate.\n",
    "<br>(laranja escuro= curva suavizada, laranja claro= curva natural)</center><br>\n",
    "\n",
    "LR_METHODS.COSINE:\n",
    "<img src=\"https://drive.google.com/uc?id=1ih5TR3qpT17WZtMl-RjIH7HK39eJ_0hb\">\n",
    "LR_METHODS.STEP:\n",
    "<img src=\"https://drive.google.com/uc?id=1p87kdvgwqcmpym9lDy_LKhsj_04ydkDM\">\n",
    "\n",
    "## OneCycle\n",
    "É um método de convergência acelerada proposto por Leslie N. Smith em 2017.\n",
    "O método varia a taxa de aprendizado e momentum da rede de forma que a taxa de aprendizado sobe até um pico e depois decresce gradualmente, por outro lado,  o momentum da rede obedece uma curvatura inversa da taxa de aprendizagem como ilustrado abaixo.\n",
    "![imagem](https://miro.medium.com/max/720/1*38YBWIKFwXN0YlNOVo_LOA.jpeg)\n",
    "\n",
    "O framework fast.ai implementa uma variação deste método e é um dos métodos mais utilizado no fast.ai. Também está disponível no framework pytorch (implementação original e do fast.ai). Este notebook utiliza-se da implementação do pytorch, deixando ao usuário a liberdade de escolha entre a variação do fast.ai ou o modo original.<br>Mais detalhes podem ser encontrados nos materias a seguir:\n",
    "* https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.OneCycleLR.html\n",
    "* A disciplined approach to neural network hyper-parameters: Part 1 — learning rate, batch size, momentum, and weight decay — https://arxiv.org/abs/1803.09820\n",
    "* Super-Convergence: Very Fast Training of Residual Networks Using Large Learning Rates — https://arxiv.org/abs/1708.07120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 240,
     "status": "ok",
     "timestamp": 1669090507378,
     "user": {
      "displayName": "Rodrigo Ribeiro",
      "userId": "10521807329412892963"
     },
     "user_tz": 180
    },
    "id": "tHFD4n3T_TyP",
    "outputId": "df546c98-2eb0-4653-dec3-43b79d9b1449"
   },
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "#-- membros estáticos\n",
    "LR_METHODS = Enum(\"LR_METHODS\", \"COSINE FIXED STEP ONECYCLE\")\n",
    "#--\n",
    "\n",
    "\"\"\"\n",
    "# CONFIGS GENERICAS\n",
    "\"\"\"\n",
    "VAL_TRAIN_LOSS = True # Ativa ou desativa o calculo de loss no conjunto val durante o treino.\n",
    "NUM_EPOCH = 1 # Define o número de épocas para rodar ex: 100\n",
    "SAVE_EPOCH = 1 # Define o intervalo de épocas para salvar, ex: de 1 em 1\n",
    "BSIZE = 8 # Define o batch_size, ex: 12\n",
    "LEARNING_RATE = 0.01 # Define a learnig rate base, ex: 0.01\n",
    "WARMUP_ITERS = 100 # Número de iterações iniciais para aquecimento da LR. Padrão: 1000\n",
    "\n",
    "# Define o método para learning rate adaptativa. \n",
    "# Atualmente disponíveis: LR_METHODS.COSINE, LR_METHODS.FIXED, LR_METHODS.STEP e LR_METHODS.ONECYCLE (ver gráficos/info no bloco acima)\n",
    "# LR_METHODS.FIXED >> mantém a learning rate fixa o tempo todo sem decaimento.\n",
    "LR_METHOD = LR_METHODS.ONECYCLE\n",
    "\n",
    "\"\"\"\n",
    "# COSINE CONFIGS (LR_METHODS.COSINE)\n",
    "\"\"\"\n",
    "# Define o momentum base para a LR no método Cosine.\n",
    "LR_MOMENTUM = 0.9\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# STEP CONFIGS (LR_METHODS.STEP)\n",
    "\"\"\"\n",
    "# Array das n% de decay em 0-1, ex: 60%, 80%, 90% = [0.6, 0.8, 0.9], primeiro decay aos 60% do conjunto treinado, segundo aos 80% e terceiro/último aos 90%.\n",
    "LR_STEPS_DECAY = [0.6, 0.8]\n",
    "# Define o quanto vai decair a Learning Rate atual em cada passo: LR_BASE = 0.1, decay1 = LR_BASE * STEP_RATE = 0.01, decay2 = NOVA_LR(0.01) * STEP_RATE = 0.001, etc..\n",
    "LR_STEP_RATE = 0.1\n",
    "\n",
    "\"\"\"\n",
    "# ONECYCLE CONFIGS (LR_METHODS.ONECYCLE)\n",
    "\"\"\"\n",
    "OC_LIMIT = 0.01 # limite maximo de crescimento da learning rate\n",
    "OC_MIN_MOMENTUM = 0.85 # momentum minimo\n",
    "OC_MAX_MOMENTUM = 0.95 # momentum maximo\n",
    "OC_THREE_PHASE = False # True para usar a versao original do Artigo. False para usar a modificação do pytorch/fast.ai (não publicado mas teoricamente melhor)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# CONFIGS PARA RESUMO DE TREINO\n",
    "\"\"\"\n",
    "RESUMIR = False # False = novo treino, ao RESUMIR AS CONFIG ACIMA SAO IGNORADAS E CARREGADAS DO ARQUIVO DO MODELO (para evitar erros por diferença)\n",
    "PASTA_RESUMO = \"27-8-2022_12_32_40\" # pasta onde foi salvo o treino a ser resumido\n",
    "\n",
    "#--msg logger--#\n",
    "if RESUMIR:\n",
    "  _msg = \"Bloco de Configuração: OK (Iniciando em modo de Resumo)\"\n",
    "else:\n",
    "  _msg = \"Bloco de Configuração: OK (Iniciando em modo Treino)\"\n",
    "\n",
    "import IPython\n",
    "js_code = \"const senderChannel = new BroadcastChannel('logger'); senderChannel.postMessage('{msg}');\".format(msg=_msg)\n",
    "display(IPython.display.Javascript(js_code))\n",
    "#--msg logger (END) --#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kttr_buA2Vgz"
   },
   "source": [
    "#1 Montar o google drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 22107,
     "status": "ok",
     "timestamp": 1669090533305,
     "user": {
      "displayName": "Rodrigo Ribeiro",
      "userId": "10521807329412892963"
     },
     "user_tz": 180
    },
    "id": "g-O6aaWTSPig",
    "outputId": "69fa1a60-873c-49ae-bf3d-fcec4f41776c"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive', force_remount=False)\n",
    "_msg = \"Montagem do Drive: OK\"\n",
    "#--msg logger--#\n",
    "import IPython\n",
    "js_code = \"const senderChannel = new BroadcastChannel('logger'); senderChannel.postMessage('{msg}');\".format(msg=_msg)\n",
    "display(IPython.display.Javascript(js_code))\n",
    "#--msg logger (END) --#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aggX8JZm2pSn"
   },
   "source": [
    "#2 Realizar o download dos arquivos necessários. \n",
    "Download e extração do dataset e demais arquivos necessários.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "executionInfo": {
     "elapsed": 3735,
     "status": "ok",
     "timestamp": 1669090552480,
     "user": {
      "displayName": "Rodrigo Ribeiro",
      "userId": "10521807329412892963"
     },
     "user_tz": 180
    },
    "id": "UdAcG4mUSvCO",
    "outputId": "9ca4ba31-915d-46e7-cda4-e9032f82e709"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "#definir onde irá salvar\n",
    "runs_path = \"/content/gdrive/MyDrive/cones_runs/\"\n",
    "os.makedirs(runs_path, exist_ok=True)\n",
    "\n",
    "if os.path.isdir('/content/dataset'):\n",
    "  _msg = \"Download e extração do dataset: Pulando etapa... (pasta dataset já existe na sessão atual)\"\n",
    "else:\n",
    "  os.chdir(\"/content/\")\n",
    "  # download cones dataset\n",
    "  !gdown 1TxbWIbi6PEaMJnEVZZ-R4TNbwSWxNLCQ\n",
    "  os.makedirs(\"/content/dataset\", exist_ok=True)\n",
    "  %mv cone_dataset.zip /content/dataset/\n",
    "  os.chdir(\"/content/dataset\")\n",
    "  !unzip -qq -u cone_dataset.zip\n",
    "  !rm -rf cone_dataset.zip\n",
    "  os.chdir(\"/content/\")\n",
    "  _msg = \"Download e extração do dataset: OK\"\n",
    "\n",
    "#--msg logger--#\n",
    "import IPython\n",
    "js_code = \"const senderChannel = new BroadcastChannel('logger'); senderChannel.postMessage('{msg}');\".format(msg=_msg)\n",
    "display(IPython.display.Javascript(js_code))\n",
    "#--msg logger (END) --#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sZu_5x9D2sW9"
   },
   "source": [
    "#3 Conversão de dados para o formato COCO.\n",
    "Realiza a conversão do formato labelme para [coco](https://cocodataset.org/#home) json. Caso não precise conversão (como é o caso do cones dataset, pule esta etapa), caso precise de outra conversão, realizar neste bloco.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 231,
     "status": "ok",
     "timestamp": 1669090565072,
     "user": {
      "displayName": "Rodrigo Ribeiro",
      "userId": "10521807329412892963"
     },
     "user_tz": 180
    },
    "id": "pR6cIO64Vv_b",
    "outputId": "fc44fe2c-1d0a-43be-fb38-2031a693ce0b"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "if RESUMIR:\n",
    "  _msg = \"Conversão do dataset para formato COCO: Pulando etapa... (modo de resumo já possui essa conversão).\"\n",
    "else:\n",
    "  #!python label2coco.py \"/content/dataset/\" --output \"coco_format.json\" #o dataset 'cone_dataset' já está vindo no formato COCO, linha mantida apenas para referência\n",
    "  _msg = \"Conversão do dataset para formato COCO: OK\"\n",
    "#--msg logger--#\n",
    "import IPython\n",
    "js_code = \"const senderChannel = new BroadcastChannel('logger'); senderChannel.postMessage('{msg}');\".format(msg=_msg)\n",
    "display(IPython.display.Javascript(js_code))\n",
    "#--msg logger (END) --#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JIHvgtnX3Ge0"
   },
   "source": [
    "#4 Divisão do dataset em train, val e test.\n",
    "É usual em tarefas de Machine Learning e Deep Learning, utilizar-se de 2 a 3 subconjuntos extraidos do dataset principal.<br>\n",
    "Normalmente são nomeados como Train, Val e Test Split. Ou seja, divisão de treino, validação e teste.<br>\n",
    "A rede é treinada com o a divisão \"train\", de forma que as divisões val e test são desconhecidas para a rede. Desta forma é possível verificar situações como \"Overfitting\", caso que ocorre quando a rede vai bem no conjunto de treino e vai mal nos conjuntos de val e test (decora os dados mas não aprende de fato).\n",
    "<br>A proporção mais utilizada é a de: 70%, 15%, 15% no caso de train/val/test ou 70%/30% (train/val).\n",
    "Este notebook utiliza-se de 3 conjuntos na proporção 70/15/15.\n",
    "Ao final do treino é realizada uma inferência no conjunto 'val' automaticamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 1112,
     "status": "ok",
     "timestamp": 1669090570208,
     "user": {
      "displayName": "Rodrigo Ribeiro",
      "userId": "10521807329412892963"
     },
     "user_tz": 180
    },
    "id": "nzmRwbA7YJLX",
    "outputId": "e1b22927-03e1-4c31-df17-42a50169cc7d"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "def save_coco(file, images, annotations, categories):\n",
    "    with open(file, 'w') as coco:\n",
    "        json.dump({ 'images': images, 'annotations': annotations, 'categories': categories}, coco, indent=2, sort_keys=True)\n",
    "\n",
    "def filter_annotations(annotations, images):\n",
    "    image_ids = map(lambda i: int(i['id']), images)\n",
    "    image_ids = list(image_ids)\n",
    "    filtered = filter(lambda a: int(a['image_id']) in image_ids, annotations)\n",
    "    return list(filtered)\n",
    "\n",
    "if RESUMIR:\n",
    "  _msg = \"Divisão train/val/test do dataset: Pulando etapa... (modo de resumo já possui estas definições)\"\n",
    "else:\n",
    "  pass # cone dataset já está dividido em train,val e test, código mantido para referência.\n",
    "  \"\"\"\n",
    "  _train = 0.7\n",
    "  _val = 0.15\n",
    "  _test = 0.15\n",
    "\n",
    "  with open('/content/dataset/coco_format.json', 'r') as infile:\n",
    "    coco = json.load(infile)\n",
    "    images = coco['images']\n",
    "    annotations = coco['annotations']\n",
    "    categories = coco['categories']\n",
    "    train_set, y = train_test_split(images, train_size=_train)\n",
    "    val_set, test_set = train_test_split(y, train_size= (_val/(_val + _test)))\n",
    "    os.chdir(\"/content/\")\n",
    "    save_coco('train.json', train_set, filter_annotations(annotations, train_set), categories)\n",
    "    save_coco('val.json', val_set, filter_annotations(annotations, val_set), categories)\n",
    "    save_coco('test.json', test_set, filter_annotations(annotations, test_set), categories)\n",
    "  \"\"\"\n",
    "  _msg = \"Divisão train/val/test do dataset: OK\"\n",
    "    \n",
    "#--msg logger--#\n",
    "import IPython\n",
    "js_code = \"const senderChannel = new BroadcastChannel('logger'); senderChannel.postMessage('{msg}');\".format(msg=_msg)\n",
    "display(IPython.display.Javascript(js_code))\n",
    "#--msg logger (END) --#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c-Rvz7TD3qQk"
   },
   "source": [
    "#5 Instalação do [Detectron2](https://github.com/facebookresearch/detectron2) direto do git oficial (última versão disponível).\n",
    "O detectron2 é um framework construído sobre o [PyTorch](https://pytorch.org/) pelo FAIR (Facebook Artificial Inteligence Research) que possibilita um alto nível de customização e uma grande variedade de modelos prontos para uso.<br>\n",
    "Para uma versão específica é necessário alterar o git alvo no código abaixo para a versão pretendida.<br>\n",
    "Para mais informações sobre o [Detectron2](https://github.com/facebookresearch/detectron2), verifique sua documentação [neste link](https://detectron2.readthedocs.io/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 219584,
     "status": "ok",
     "timestamp": 1669090794495,
     "user": {
      "displayName": "Rodrigo Ribeiro",
      "userId": "10521807329412892963"
     },
     "user_tz": 180
    },
    "id": "IywpOM5_w81c",
    "outputId": "d6fa4483-8213-485d-a3aa-0b1cd9480bc9"
   },
   "outputs": [],
   "source": [
    "!python -m pip install pyyaml==5.1\n",
    "!python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'\n",
    "#--msg logger--#\n",
    "_msg = \"Instalação do Detectron2: OK\"\n",
    "import IPython\n",
    "js_code = \"const senderChannel = new BroadcastChannel('logger'); senderChannel.postMessage('{msg}');\".format(msg=_msg)\n",
    "display(IPython.display.Javascript(js_code))\n",
    "#--msg logger (END) --#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iIcIXGmr3ww4"
   },
   "source": [
    "#6 Verificação da instalação e versão do Detectron2 e do PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1249,
     "status": "ok",
     "timestamp": 1669090795737,
     "user": {
      "displayName": "Rodrigo Ribeiro",
      "userId": "10521807329412892963"
     },
     "user_tz": 180
    },
    "id": "2G2lz1eg4a_J",
    "outputId": "510e4241-13f5-4ce8-ec55-1e885a502817"
   },
   "outputs": [],
   "source": [
    "#ver info da maquina\n",
    "import torch, detectron2\n",
    "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
    "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
    "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n",
    "print(\"detectron2:\", detectron2.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ySVWSMpP32Bk"
   },
   "source": [
    "#7 Treinar a rede.\n",
    "Ao final do treino é realizado inferência no conjunto 'val'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "poAAmdcMOWdd"
   },
   "outputs": [],
   "source": [
    "import IPython\n",
    "from IPython.display import display, HTML, Javascript\n",
    "\n",
    "import torch\n",
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.evaluation import COCOEvaluator\n",
    "from detectron2.utils.events import get_event_storage\n",
    "import detectron2.utils.comm as comm\n",
    "from detectron2.engine import HookBase\n",
    "from detectron2.solver.build import build_lr_scheduler, build_optimizer, get_default_optimizer_params\n",
    "from detectron2.solver.lr_scheduler import WarmupCosineLR, WarmupMultiStepLR, LRMultiplier, WarmupParamScheduler\n",
    "from detectron2.data import DatasetMapper, build_detection_test_loader\n",
    "from fvcore.common.param_scheduler import CosineParamScheduler, MultiStepParamScheduler, StepWithFixedGammaParamScheduler\n",
    "from collections import defaultdict\n",
    "import neptune.new as neptune\n",
    "import wandb\n",
    "import os\n",
    "\n",
    "class SaveBestLoss(HookBase):\n",
    "    \"\"\"\n",
    "    Hook para salvar peso com o melhor loss de treino\n",
    "    \"\"\"\n",
    "    def __init__(self, period=20):\n",
    "        super().__init__()\n",
    "        self._period = period\n",
    "        self._bestloss = 10\n",
    "        self._last = -1\n",
    "\n",
    "    def after_step(self):\n",
    "        if (self.trainer.iter + 1) % self._period == 0 or (self.trainer.iter == self.trainer.max_iter - 1):\n",
    "            self.check_model()\n",
    "    \n",
    "    def after_train(self):\n",
    "        self.check_model()\n",
    "    \n",
    "    def check_model(self):\n",
    "        storage = get_event_storage()\n",
    "        allmetrics = defaultdict(dict)\n",
    "        for k, (v, iter) in storage.latest_with_smoothing_hint(self._period).items():\n",
    "            if iter <= self._last:\n",
    "                continue\n",
    "            allmetrics[iter][k] = v\n",
    "        if len(allmetrics):\n",
    "            all_iters = sorted(allmetrics.keys())\n",
    "            self._last = max(all_iters)\n",
    "            if \"total_loss\" in allmetrics[self._last].keys():\n",
    "                if allmetrics[self._last][\"total_loss\"] < self._bestloss:\n",
    "                    self._bestloss = allmetrics[self._last][\"total_loss\"]\n",
    "                    self.trainer.checkpointer.save(\"model_best_loss\")\n",
    "        \n",
    "\n",
    "class ValidationLoss(HookBase):\n",
    "    \"\"\"\n",
    "    Hook para calcular loss durante o treinamento com o set val a cada 20 iterações\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg, period=20):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg.clone()\n",
    "        self._period = period\n",
    "        self._loader = iter(build_detection_test_loader(self.cfg, self.cfg.DATASETS.TEST, mapper=DatasetMapper(self.cfg, is_train=True)))\n",
    "\n",
    "    def after_step(self):\n",
    "        data = next(self._loader)\n",
    "        if (self.trainer.iter + 1) % self._period == 0 or (self.trainer.iter == self.trainer.max_iter - 1):\n",
    "            with torch.no_grad():\n",
    "                loss_dict = self.trainer.model(data)\n",
    "                losses = sum(loss_dict.values())\n",
    "                assert torch.isfinite(losses).all(), loss_dict\n",
    "                loss_dict_reduced = {\"validation_\" + k: v.item() for k, v in comm.reduce_dict(loss_dict).items()}\n",
    "                losses_reduced = sum(loss for loss in loss_dict_reduced.values())\n",
    "                if comm.is_main_process():\n",
    "                    self.trainer.storage.put_scalars(validation_total_loss=losses_reduced, **loss_dict_reduced)\n",
    "\n",
    "class ExternalLogger(HookBase):\n",
    "    \"\"\"\n",
    "    Hook para enviar os dados de treino para o WandB.ai e/ou Neptune.ai\n",
    "    \"\"\"\n",
    "    def __init__(self, period=20, npt=False, wndb=False, resume=False, maxiter = 0):\n",
    "        super().__init__()\n",
    "        self._period = period\n",
    "        self._npt = npt\n",
    "        self._wndb = wndb\n",
    "        self._resume = resume\n",
    "        self._last = -1\n",
    "        self._params = {\"batch_size\": BSIZE, \"epochs\": NUM_EPOCH, \"learning_rate\": LEARNING_RATE, \"scheduler\": LR_METHOD.name, \"iterations\": maxiter}\n",
    "        if self._npt:\n",
    "            if self._resume:\n",
    "                self._npt_run = neptune.init_run(project=NPT_PROJECT_NAME, api_token=NPT_TOKEN, with_id=NPT_RUN_ID)\n",
    "            else:\n",
    "                self._npt_run = neptune.init_run(project=NPT_PROJECT_NAME, api_token=NPT_TOKEN)\n",
    "        if self._wndb:\n",
    "            os.environ[\"WANDB_SILENT\"] = \"True\"\n",
    "            os.environ[\"WANDB_API_KEY\"] = WANDB_API_KEY\n",
    "            if self._resume:\n",
    "                self._wnd_run = wandb.init(project=WANDB_PROJECT_NAME, entity=WANDB_ENTITY, id=WANDB_RUN_ID, resume=\"must\")\n",
    "            else:\n",
    "                self._wnd_run = wandb.init(project=WANDB_PROJECT_NAME, entity=WANDB_ENTITY)\n",
    "        if not self._resume:\n",
    "            self._npt_run[\"parameters\"] = self._params\n",
    "            wandb.config.update(self._params)\n",
    "\n",
    "    def after_step(self):\n",
    "        if (self.trainer.iter + 1) % self._period == 0 or (self.trainer.iter == self.trainer.max_iter - 1):\n",
    "            storage = get_event_storage()\n",
    "            _iter = storage.iter\n",
    "            self._last = _iter\n",
    "            for k, (v, iter) in storage.latest_with_smoothing_hint(self._period).items():\n",
    "                if self._wndb:\n",
    "                    if self._resume:\n",
    "                        if wandb.run.step < _iter:\n",
    "                            wandb.log({f'{k.replace(\"/\", \"-\")}': v}, step=_iter)\n",
    "                    else:\n",
    "                        wandb.log({f'{k.replace(\"/\", \"-\")}': v}, step=_iter)\n",
    "                if self._npt:\n",
    "                    if self._resume:\n",
    "                        if self._npt_run[\"last_iter\"].fetch() < _iter:\n",
    "                            self._npt_run[k.replace(\"/\", \"-\")].log(v, step=_iter)\n",
    "                    else:\n",
    "                        self._npt_run[k.replace(\"/\", \"-\")].log(v, step=_iter)\n",
    "                        self._npt_run[\"last_iter\"] = _iter\n",
    "                    \n",
    "    def after_train(self):\n",
    "        storage = get_event_storage()\n",
    "        for k, (v, iter) in storage.latest_with_smoothing_hint(self._period).items():\n",
    "            if iter <= self._last:\n",
    "                continue\n",
    "            if self._wndb:\n",
    "                wandb.log({f'{k.replace(\"/\", \"-\")}': v}, step=iter)\n",
    "            if self._npt:\n",
    "                self._npt_run[k.replace(\"/\", \"-\")].log(v, step=iter)\n",
    "        if self._wndb:\n",
    "            self._wnd_run.finish()\n",
    "        if self._npt:\n",
    "            self._npt_run.stop()\n",
    "\n",
    "\n",
    "\n",
    "class LossLoggerJS(HookBase):\n",
    "    \"\"\"\n",
    "    Atualiza gráfico de loss no notebook em tempo real via javascript\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, period=20):\n",
    "        super().__init__()\n",
    "        self._period = period\n",
    "\n",
    "    def after_step(self):\n",
    "        if (self.trainer.iter + 1) % self._period == 0 or (self.trainer.iter == self.trainer.max_iter - 1):\n",
    "            storage = get_event_storage()\n",
    "            _iter = storage.iter\n",
    "            self._last = _iter\n",
    "            for k, (v, iter) in storage.latest_with_smoothing_hint(self._period).items():\n",
    "                if k == \"total_loss\":\n",
    "                  total_loss =  v\n",
    "            out_msg = \"{iter},{loss}\".format(iter=_iter, loss=total_loss)\n",
    "            js_code = \"const senderChannel = new BroadcastChannel('losslogger'); senderChannel.postMessage('{msg}');\".format(msg=out_msg)\n",
    "            display(IPython.display.Javascript(js_code))\n",
    "            \n",
    "    def after_train(self):\n",
    "        pass\n",
    "\n",
    "class DermaTrainer(DefaultTrainer):\n",
    "    @classmethod\n",
    "    def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n",
    "        if output_folder is None:\n",
    "            os.makedirs(cfg.OUTPUT_DIR+\"/coco_val\", exist_ok=True)\n",
    "            output_folder = cfg.OUTPUT_DIR+\"/coco_val\"\n",
    "        return COCOEvaluator(dataset_name=dataset_name, use_fast_impl=False, output_dir=output_folder)\n",
    "    \n",
    "    @classmethod\n",
    "    def build_lr_scheduler(cls, cfg, optimizer):\n",
    "        name = cfg.SOLVER.LR_SCHEDULER_NAME\n",
    "        if name == \"WarmupMultiStepLR\":\n",
    "            steps = [x for x in cfg.SOLVER.STEPS if x <= cfg.SOLVER.MAX_ITER]\n",
    "            if len(steps) != len(cfg.SOLVER.STEPS):\n",
    "                logger = logging.getLogger(__name__)\n",
    "                logger.warning(\"SOLVER.STEPS contains values larger than SOLVER.MAX_ITER. These values will be ignored.\")\n",
    "            sched = MultiStepParamScheduler(\n",
    "              values=[cfg.SOLVER.GAMMA**k for k in range(len(steps) + 1)],\n",
    "              milestones=steps,\n",
    "              num_updates=cfg.SOLVER.MAX_ITER,\n",
    "            )\n",
    "        elif name == \"WarmupCosineLR\":\n",
    "            end_value = cfg.SOLVER.BASE_LR_END / cfg.SOLVER.BASE_LR\n",
    "            assert end_value >= 0.0 and end_value <= 1.0, end_value\n",
    "            sched = CosineParamScheduler(1, end_value)\n",
    "        elif name == \"WarmupStepWithFixedGammaLR\":\n",
    "            sched = StepWithFixedGammaParamScheduler(\n",
    "              base_value=1.0,\n",
    "              gamma=cfg.SOLVER.GAMMA,\n",
    "              num_decays=cfg.SOLVER.NUM_DECAYS,\n",
    "              num_updates=cfg.SOLVER.MAX_ITER,\n",
    "            )\n",
    "        elif name == \"OneCycleLR\":\n",
    "            return torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=cfg.ONECYCLE.LIMIT, base_momentum=cfg.ONECYCLE.MIN_MOMENTUM, max_momentum=cfg.ONECYCLE.MAX_MOMENTUM, total_steps=cfg.SOLVER.MAX_ITER, three_phase=cfg.ONECYCLE.THREE_PHASE)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown LR scheduler: {}\".format(name))\n",
    "\n",
    "        sched = WarmupParamScheduler(\n",
    "          sched,\n",
    "          cfg.SOLVER.WARMUP_FACTOR,\n",
    "          min(cfg.SOLVER.WARMUP_ITERS / cfg.SOLVER.MAX_ITER, 1.0),\n",
    "          cfg.SOLVER.WARMUP_METHOD,\n",
    "          cfg.SOLVER.RESCALE_INTERVAL,\n",
    "        )\n",
    "        return LRMultiplier(optimizer, multiplier=sched, max_iter=cfg.SOLVER.MAX_ITER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para mais modelos consulte: <a href=https://github.com/facebookresearch/detectron2/blob/main/MODEL_ZOO.md>Detectron2 Model Zoo</a>\n",
    "\n",
    "Caso deseje trocar o modelo em uso altere as seguintes linhas:<br>\n",
    "<code>cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"))<br>\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\") </code><br>\n",
    "\n",
    "Para rodar em um dataset customizado, é necessário alterar o número de classes da última camada, alterando a seguinte linha:<br>\n",
    "<code>cfg.MODEL.ROI_HEADS.NUM_CLASSES = 6 # numero de classes do dataset</code><br>\n",
    "\n",
    "<b>Para outros modelos consulte a documentação do Detectron2 para quais atributos adicionais são necessários.</b>\n",
    "\n",
    "O Detectron2 espera dataset no formato coco com um arquivo json para cada split (train, test, val), as imagens podem estar na mesma pasta.<br>\n",
    "Para adicionar um dataset customizado, procure e edite as seguintes linhas:<br>\n",
    "<code>\n",
    "register_coco_instances(\"instance_train\", {}, \"/content/dataset/simple_street_segmentation/train/train.json\", \"/content/dataset/simple_street_segmentation/train\")<br>\n",
    "register_coco_instances(\"instance_val\", {}, \"/content/dataset/simple_street_segmentation/valid/valid.json\", \"/content/dataset/simple_street_segmentation/valid\")<br>\n",
    "</code>\n",
    "\n",
    "O formato para registrar um dataset com a função register_coco_instances segue abaixo:<br>\n",
    "<code>register_coco_instances(\"nome_para_localizar\", {}, \"caminho para o arquivo json\", \"caminho para as imagens\")</code><br>\n",
    "\n",
    "Para adicioanr ao CFG, procure e altere as seguintes linhas:<br>\n",
    "<code>cfg.DATASETS.TRAIN = (\"instance_train\",)<br>\n",
    "cfg.DATASETS.TEST = (\"instance_val\",)</code><br>\n",
    "\n",
    "Altere para:<br>\n",
    "<code>cfg.DATASETS.TRAIN = (\"nome_registrado_para_spli_train\",)<br>\n",
    "cfg.DATASETS.TEST = (\"nome_registrado_para_spli_validation\",)</code><br>\n",
    "\n",
    "Outras opções estão comentadas no código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 797053,
     "status": "ok",
     "timestamp": 1669091594758,
     "user": {
      "displayName": "Rodrigo Ribeiro",
      "userId": "10521807329412892963"
     },
     "user_tz": 180
    },
    "id": "_DxNHSCdxSln",
    "outputId": "b9fe2d0b-55b0-4e82-a60e-32242871322a"
   },
   "outputs": [],
   "source": [
    "import detectron2\n",
    "from detectron2.utils.logger import setup_logger\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import pytz\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from google.colab.patches import cv2_imshow\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.config import CfgNode\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "from detectron2.data.datasets import register_coco_instances\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import PeriodicWriter\n",
    "\n",
    "import logging\n",
    "\n",
    "\n",
    "if RESUMIR:\n",
    "  #--msg logger--#\n",
    "  _msg = \"Iniciando treino: Pulando etapa... (modo de resumo selecionado)\"\n",
    "  import IPython\n",
    "  js_code = \"const senderChannel = new BroadcastChannel('logger'); senderChannel.postMessage('{msg}');\".format(msg=_msg)\n",
    "  display(IPython.display.Javascript(js_code))\n",
    "else:\n",
    "  time_now = datetime.datetime.now(pytz.timezone(\"America/Sao_Paulo\"))\n",
    "  _msg = \"Iniciando treino: OK (treino iniciado em {dia}/{mes}/{ano} as {hora}:{minuto}:{segundo})\".format(dia=time_now.day, mes=time_now.month, ano=time_now.year, hora=time_now.hour, minuto=time_now.minute, segundo=time_now.second)\n",
    "  import IPython\n",
    "  js_code = \"const senderChannel = new BroadcastChannel('logger'); senderChannel.postMessage('{msg}');\".format(msg=_msg)\n",
    "  display(IPython.display.Javascript(js_code))\n",
    "  #--msg logger (END) --#\n",
    "\n",
    "  setup_logger()  \n",
    "  \n",
    "  cfg = get_cfg()\n",
    "  cfg.ONECYCLE = CfgNode({\"LIMIT\": 0.01, \"MAX_MOMENTUM\": 0.95, \"MIN_MOMENTUM\": 0.85, \"THREE_PHASE\": False})\n",
    "  \n",
    "  cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")) \n",
    "  cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\") \n",
    "  \n",
    "  ##-- Definir os caminhos dos datasets\n",
    "  DatasetCatalog.clear()\n",
    "  MetadataCatalog.clear()\n",
    "  register_coco_instances(\"cone_train\", {}, \"/content/dataset/train/_annotations.coco.json\", \"/content/dataset/train\")\n",
    "  register_coco_instances(\"cone_val\", {}, \"/content/dataset/valid/_annotations.coco.json\", \"/content/dataset/valid\")\n",
    "  cfg.DATASETS.TRAIN = (\"cone_train\",)\n",
    "  cfg.DATASETS.TEST = (\"cone_val\",)\n",
    "  ##-- END definir caminhos datasets\n",
    "\n",
    "  ##---------CONFIGURACOES COMEÇAM DAQUI PARA BAIXO ------------------------##\n",
    "  cfg.DATALOADER.NUM_WORKERS = 2 # numero de subprocessos para usar no dataloader para pré-carregar as imagens (via cpu). Depende da quantidade de cores do cpu e da memoria disponível no sistema. [2-4]\n",
    "  cfg.SOLVER.BASE_LR = LEARNING_RATE\n",
    "  cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 512 # qtos rois da RPN vão ser usados para verificar region/classification loss durante o treino, ideal multiplo de 2.\n",
    "  cfg.MODEL.ROI_HEADS.NUM_CLASSES = 6 # numero de classes do nosso dataset\n",
    "  cfg.SOLVER.IMS_PER_BATCH = BSIZE # usei 12. batch 16  nao rolou, deu out of memory (Tesla T4)(15Gb) * deixei ja 16 como o professor pediu.\n",
    "\n",
    "  #Calculando iteracao em epoch\n",
    "  NR_IMG = len(DatasetCatalog.get('cone_train'))\n",
    "  EPOCH = NR_IMG / cfg.SOLVER.IMS_PER_BATCH\n",
    "  cfg.SOLVER.MAX_ITER = int(NUM_EPOCH * EPOCH) #\n",
    "  cfg.SOLVER.WARMUP_ITERS = WARMUP_ITERS\n",
    "  if LR_METHOD == LR_METHODS.COSINE:\n",
    "    cfg.SOLVER.LR_SCHEDULER_NAME = \"WarmupCosineLR\"\n",
    "    cfg.SOLVER.MOMENTUM = LR_MOMENTUM\n",
    "  elif LR_METHOD == LR_METHODS.STEP:\n",
    "    cfg.SOLVER.LR_SCHEDULER_NAME = \"WarmupMultiStepLR\"\n",
    "    cfg.SOLVER.GAMMA = LR_STEP_RATE\n",
    "    cfg.SOLVER.STEPS = tuple( [int(cfg.SOLVER.MAX_ITER*LR_STEPS_DECAY[i]) for i in range(len(LR_STEPS_DECAY))])\n",
    "  elif LR_METHOD == LR_METHODS.FIXED:\n",
    "    cfg.SOLVER.LR_SCHEDULER_NAME = \"WarmupMultiStepLR\"\n",
    "    cfg.SOLVER.STEPS = []\n",
    "  elif LR_METHOD == LR_METHODS.ONECYCLE:\n",
    "    cfg.SOLVER.LR_SCHEDULER_NAME = \"OneCycleLR\"\n",
    "    cfg.ONECYCLE.LIMIT = OC_LIMIT\n",
    "    cfg.ONECYCLE.MIN_MOMENTUM = OC_MIN_MOMENTUM\n",
    "    cfg.ONECYCLE.MAX_MOMENTUM = OC_MAX_MOMENTUM\n",
    "    cfg.ONECYCLE.THREE_PHASE = OC_THREE_PHASE\n",
    "    cfg.SOLVER.WARMUP_ITERS = 0\n",
    "    cfg.SOLVER.STEPS = []\n",
    "  else:\n",
    "    pass\n",
    "\n",
    "  EPOCH_RATIO = NUM_EPOCH / SAVE_EPOCH\n",
    "  # checkpoint period faz o dt salvar o modelo atual a cada x iterações definidas aqui\n",
    "  cfg.SOLVER.CHECKPOINT_PERIOD = int(cfg.SOLVER.MAX_ITER/EPOCH_RATIO) # \n",
    "\n",
    "  #cfg.TEST.EVAL_PERIOD = 1000 # nao necessario agora, periodo o qual durante o treino ele vai fazer um eval com o conjunto \"derma_val\" para acompanhamento apenas\n",
    "  \n",
    "  ##-------------END CONFG -------###\n",
    "\n",
    "  #config de dir\n",
    "  current_run = datetime.datetime.now(pytz.timezone(\"America/Sao_Paulo\"))\n",
    "  dirname = \"{dia}-{mes}-{ano}_{hora}_{minuto}_{segundo}\".format(dia=current_run.day, mes=current_run.month, ano=current_run.year, hora=current_run.hour, minuto=current_run.minute, segundo=current_run.second)\n",
    "  outdir = runs_path+dirname\n",
    "  os.makedirs(outdir, exist_ok=True)\n",
    "  cfg.OUTPUT_DIR = outdir\n",
    "   \n",
    "\n",
    "  with open(outdir + '/' + 'faster_R_50.yml','w') as f:\n",
    "    f.write(cfg.dump())\n",
    "\n",
    "  trainer = DermaTrainer(cfg)\n",
    "  \n",
    "  if VAL_TRAIN_LOSS:\n",
    "    trainer.register_hooks([ValidationLoss(cfg)])\n",
    "    pw_hook = [hook for hook in trainer._hooks if isinstance(hook, PeriodicWriter)]\n",
    "    all_hooks = [hook for hook in trainer._hooks if not isinstance(hook, PeriodicWriter)]\n",
    "    trainer._hooks = all_hooks + pw_hook\n",
    "  if PLOT_LOSS:\n",
    "    js_hook = LossLoggerJS()  \n",
    "    trainer.register_hooks([js_hook])\n",
    "  if WANDB_LOGGER or NPT_LOGGER:\n",
    "    exlogger_hook = ExternalLogger(npt=NPT_LOGGER, wndb=WANDB_LOGGER, resume=False, maxiter=cfg.SOLVER.MAX_ITER)  \n",
    "    trainer.register_hooks([exlogger_hook])  \n",
    "  best_hook = SaveBestLoss()\n",
    "  trainer.register_hooks([best_hook])\n",
    "  trainer.resume_or_load(resume=False)\n",
    "  trainer.train()\n",
    "\n",
    "  #--msg logger--#\n",
    "  time_now = datetime.datetime.now(pytz.timezone(\"America/Sao_Paulo\"))\n",
    "  _msg = \"Treino terminado em {dia}/{mes}/{ano} as {hora}:{minuto}:{segundo})\".format(dia=time_now.day, mes=time_now.month, ano=time_now.year, hora=time_now.hour, minuto=time_now.minute, segundo=time_now.second)\n",
    "  import IPython\n",
    "  js_code = \"const senderChannel = new BroadcastChannel('logger'); senderChannel.postMessage('{msg}');\".format(msg=_msg)\n",
    "  display(IPython.display.Javascript(js_code))\n",
    "  #--msg logger (END) --#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AOTKbp-W4LvC"
   },
   "source": [
    "#8 Realizar inferência com o conjunto test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 49067,
     "status": "ok",
     "timestamp": 1669091656931,
     "user": {
      "displayName": "Rodrigo Ribeiro",
      "userId": "10521807329412892963"
     },
     "user_tz": 180
    },
    "id": "4TrCXxCsxUGs",
    "outputId": "f3a10d34-769d-4d7f-f0fe-db3a97a5868f"
   },
   "outputs": [],
   "source": [
    "from detectron2.config import CfgNode\n",
    "from detectron2.data import build_detection_test_loader\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "import torch\n",
    "cfg_test = get_cfg()\n",
    "cfg_test.ONECYCLE = CfgNode({\"LIMIT\": 0.01, \"MAX_MOMENTUM\": 0.95, \"MIN_MOMENTUM\": 0.85, \"THREE_PHASE\": False})\n",
    "cfg_test.merge_from_file(outdir + \"/faster_R_50.yml\")\n",
    "cfg_test.MODEL.WEIGHTS = outdir + \"/model_final.pth\"\n",
    "with torch.no_grad():\n",
    "  predictor = DefaultPredictor(cfg_test)\n",
    "  model = predictor.model\n",
    "  register_coco_instances(\"cone_test\", {}, \"/content/dataset/test/_annotations.coco.json\", \"/content/dataset/test\")\n",
    "  os.makedirs(\"./coco_test\", exist_ok=True)\n",
    "  evaluator = COCOEvaluator(\"cone_test\", output_dir=\"./coco_test\")\n",
    "  test_loader = build_detection_test_loader(cfg_test, \"cone_test\")\n",
    "  print(inference_on_dataset(model, test_loader, evaluator))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CQddVzjonSG4"
   },
   "source": [
    "# 9 Visualização das Detecções no conjunto test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 5149,
     "status": "ok",
     "timestamp": 1669093041926,
     "user": {
      "displayName": "Rodrigo Ribeiro",
      "userId": "10521807329412892963"
     },
     "user_tz": 180
    },
    "id": "KiMqW4OOnXzB",
    "outputId": "e2f32b1d-3b0f-41e5-95e8-b472b83aa1b6"
   },
   "outputs": [],
   "source": [
    "import os, json, cv2, random\n",
    "from google.colab.patches import cv2_imshow\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "cfg_test = get_cfg()\n",
    "cfg_test.ONECYCLE = CfgNode({\"LIMIT\": 0.01, \"MAX_MOMENTUM\": 0.95, \"MIN_MOMENTUM\": 0.85, \"THREE_PHASE\": False})\n",
    "cfg_test.merge_from_file(outdir + \"/faster_R_50.yml\")\n",
    "cfg_test.MODEL.WEIGHTS = outdir + \"/model_final.pth\"\n",
    "dataset_dicts = DatasetCatalog.get('cone_test')\n",
    "num_samples = 4 # número de amostra para visualizar\n",
    "with torch.no_grad():\n",
    "  predictor = DefaultPredictor(cfg_test)\n",
    "  model = predictor.model\n",
    "  for det in random.sample(dataset_dicts, num_samples):\n",
    "    img = cv2.imread(det[\"file_name\"])\n",
    "    outputs = predictor(img) \n",
    "    v = Visualizer(img[:, :, ::-1], metadata=MetadataCatalog.get('cone_test'), scale=1.75)\n",
    "    out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "    cv2_imshow(out.get_image()[:, :, ::-1])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aYRR5IYf4i9K"
   },
   "source": [
    "#10 Visualização dos dados de treino(plot) e TensorBoard(opcional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P59Y34WeOoHB"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "metric_file = outdir+\"/metrics.json\"\n",
    "metrics = []\n",
    "with open(metric_file, 'r') as infile:\n",
    "  for line in infile.readlines():\n",
    "    metrics.append(json.loads(line))\n",
    "\n",
    "APmetrics = metrics[-1] if \"bbox/AP\" in metrics[-1].keys() else None\n",
    "if APmetrics is not None:\n",
    "  metrics.pop(-1)\n",
    "  APmetrics.pop('iteration', None) # não precisamos de iteration nos AP finais\n",
    "\n",
    "mt = dict()\n",
    "for k in metrics[0].keys():\n",
    "  _temp = [metrics[i][k] for i in range(len(metrics))]\n",
    "  mt.update({k: _temp})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 213,
     "status": "ok",
     "timestamp": 1669091687634,
     "user": {
      "displayName": "Rodrigo Ribeiro",
      "userId": "10521807329412892963"
     },
     "user_tz": 180
    },
    "id": "eLoW8Q8ZBAPR",
    "outputId": "45b3761a-b823-4f04-8b3a-ada04c89b164"
   },
   "outputs": [],
   "source": [
    "print(\"Keys válidas:\", mt.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 682
    },
    "executionInfo": {
     "elapsed": 1916,
     "status": "ok",
     "timestamp": 1669091691534,
     "user": {
      "displayName": "Rodrigo Ribeiro",
      "userId": "10521807329412892963"
     },
     "user_tz": 180
    },
    "id": "2SL1IO9zPcKM",
    "outputId": "23b9fc68-ca0f-4646-fa73-919ac7aec042"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "keys = [\"total_loss\", \"lr\", \"fast_rcnn/false_negative\", \"fast_rcnn/cls_accuracy\"] # adicionar keys válidas para mais plots.\n",
    "\n",
    "fig = plt.figure(figsize=(15,6), dpi=300)\n",
    "fig.subplots_adjust(hspace=0.7, wspace=0.4)\n",
    "row_index = 2\n",
    "line_index = (len(keys)//row_index)+1 if len(keys)%row_index != 0 else len(keys)//row_index\n",
    "plotcount = 1\n",
    "for k in keys:\n",
    "  _ax = fig.add_subplot(line_index, row_index, plotcount)\n",
    "  _ax.set_yscale('linear')\n",
    "  _ax.plot(mt[\"iteration\"], mt[k], linewidth=2.0)\n",
    "  _ax.set_xlabel('Iterations', fontsize=10)\n",
    "  _ax.set_title(k, fontsize=10)\n",
    "  plt.grid()\n",
    "  plotcount +=1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 426
    },
    "executionInfo": {
     "elapsed": 232,
     "status": "ok",
     "timestamp": 1669091708552,
     "user": {
      "displayName": "Rodrigo Ribeiro",
      "userId": "10521807329412892963"
     },
     "user_tz": 180
    },
    "id": "mmXfjBZmP6e5",
    "outputId": "a1c0ff35-3e9a-4e38-e818-857d897d23f0"
   },
   "outputs": [],
   "source": [
    "#-- Visualizar inferência AP como tabela --\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "if APmetrics is not None:\n",
    "  df = pd.DataFrame.from_dict(APmetrics, orient='index') # key como linhas \n",
    "  #df = pd.DataFrame(APmetrics, index=[0,]) # key como colunas\n",
    "  display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xh06pjYq__o_"
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "#comentar a linha acima com \"#\" para executar a visualização com tensorboard no diretório de treino.\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir \"$outdir\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DvTSRn9UDLs5"
   },
   "source": [
    "#11 Bloco para resumir um treino parado anteriormente [RESUMIR = True]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PJ1W7zeEDgxC"
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import time\n",
    "import datetime\n",
    "import pytz\n",
    "if RESUMIR:\n",
    "  pasta_para_resumir = PASTA_RESUMO\n",
    "  if pasta_para_resumir == \"\":\n",
    "    all_runs = glob(runs_path+\"*/\")\n",
    "    lrt = len(runs_path)\n",
    "    last_date = None\n",
    "    last_run = None\n",
    "    last_time = None\n",
    "    for _run in all_runs:\n",
    "      _date, _time = _run[lrt:-1].split('2022')\n",
    "      run_date = time.strptime(_date + '2022', \"%d-%m-%Y\")\n",
    "      run_hour, run_min, run_sec = _time.strip('_').split('_')\n",
    "      run_time = run_hour*60 + run_min*60 + run_sec\n",
    "      if last_date is None:\n",
    "        last_date = run_date\n",
    "        last_run = _run\n",
    "        last_time = run_time\n",
    "      elif run_date > last_date:\n",
    "        lastaadptação_date = run_date\n",
    "        last_run = _run\n",
    "        last_time = run_time\n",
    "      elif run_date == last_date:\n",
    "        if run_time > last_time:\n",
    "          last_date = run_date\n",
    "          last_run = _run\n",
    "          last_time = run_time\n",
    "      else:\n",
    "        pass\n",
    "    pasta_para_resumir = last_run \n",
    "  else:\n",
    "    pasta_para_resumir = runs_path + PASTA_RESUMO + \"/\"\n",
    "  \n",
    "  print(\"Resumindo do diretorio:\", pasta_para_resumir)\n",
    "  models = glob(pasta_para_resumir+\"*.pth\")\n",
    "  selected_model = None\n",
    "  aux = 0\n",
    "  for m in models:\n",
    "    numbers = ''.join([x for x in m if x.isdigit()])\n",
    "    if int(numbers) > aux:\n",
    "      selected_model = m\n",
    "      aux = int(numbers)\n",
    "  print(\"Resumindo do modelo:\", selected_model)\n",
    "  time_now = datetime.datetime.now(pytz.timezone(\"America/Sao_Paulo\"))\n",
    "  #--msg logger--#\n",
    "  _msg = \"Iniciando configurações de resumo: OK\"\n",
    "  import IPython\n",
    "  js_code = \"const senderChannel = new BroadcastChannel('logger'); senderChannel.postMessage('{msg}');\".format(msg=_msg)\n",
    "  display(IPython.display.Javascript(js_code))\n",
    "  #--msg logger (END) --#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FoxV1la6DK6f"
   },
   "outputs": [],
   "source": [
    "import detectron2\n",
    "from detectron2.utils.logger import setup_logger\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import pytz\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.data.datasets import register_coco_instances\n",
    "from detectron2 import model_zoo\n",
    "\n",
    "if RESUMIR:\n",
    "  #--msg logger--#\n",
    "  time_now = datetime.datetime.now(pytz.timezone(\"America/Sao_Paulo\"))\n",
    "  _msg = \"Iniciando resumo de treino: OK (resumo iniciado em {dia}/{mes}/{ano} as {hora}:{minuto}:{segundo})\".format(dia=time_now.day, mes=time_now.month, ano=time_now.year, hora=time_now.hour, minuto=time_now.minute, segundo=time_now.second)\n",
    "  import IPython\n",
    "  js_code = \"const senderChannel = new BroadcastChannel('logger'); senderChannel.postMessage('{msg}');\".format(msg=_msg)\n",
    "  display(IPython.display.Javascript(js_code))\n",
    "  #--msg logger (END) --#\n",
    "  \n",
    "  setup_logger()\n",
    "  \n",
    "  runs_path = pasta_para_resumir # definindo run path de resumo\n",
    "  cfg = get_cfg()\n",
    "  cfg.ONECYCLE = CfgNode({\"LIMIT\": 0.01, \"MAX_MOMENTUM\": 0.95, \"MIN_MOMENTUM\": 0.85, \"THREE_PHASE\": False})\n",
    "  cfg.merge_from_file(runs_path+\"faster_R_50.yml\") \n",
    "  cfg.MODEL.WEIGHTS = selected_model \n",
    "  register_coco_instances(\"cone_train\", {}, \"/content/dataset/train/_annotations.coco.json\", \"/content/dataset/train\")\n",
    "  register_coco_instances(\"cone_val\", {}, \"/content/dataset/valid/_annotations.coco.json\", \"/content/dataset/valid\")\n",
    "  print(\"Executando: \" + str(cfg.SOLVER.MAX_ITER) + \" iterations.\")\n",
    "  trainer = DermaTrainer(cfg)\n",
    "  if VAL_TRAIN_LOSS:\n",
    "    trainer.register_hooks([ValidationLoss(cfg)])\n",
    "    pw_hook = [hook for hook in trainer._hooks if isinstance(hook, PeriodicWriter)]\n",
    "    all_hooks = [hook for hook in trainer._hooks if not isinstance(hook, PeriodicWriter)]\n",
    "    trainer._hooks = all_hooks + pw_hook\n",
    "  if PLOT_LOSS:\n",
    "    js_hook = LossLoggerJS()  \n",
    "    trainer.register_hooks([js_hook])\n",
    "  if WANDB_LOGGER or NPT_LOGGER:\n",
    "    exlogger_hook = ExternalLogger(npt=NPT_LOGGER, wndb=WANDB_LOGGER, resume=True)  \n",
    "    trainer.register_hooks([exlogger_hook])\n",
    "  trainer.resume_or_load(resume=True) # True para resumir do checkpoint encontrado ou do melhor peso encontrado.\n",
    "  trainer.train()\n",
    "  time_now = datetime.datetime.now(pytz.timezone(\"America/Sao_Paulo\"))\n",
    "  \n",
    "  #--msg logger--#\n",
    "  _msg = \"Resumo de treino finalizado em {dia}/{mes}/{ano} as {hora}:{minuto}:{segundo})\".format(dia=time_now.day, mes=time_now.month, ano=time_now.year, hora=time_now.hour, minuto=time_now.minute, segundo=time_now.second)\n",
    "  import IPython\n",
    "  js_code = \"const senderChannel = new BroadcastChannel('logger'); senderChannel.postMessage('{msg}');\".format(msg=_msg)\n",
    "  display(IPython.display.Javascript(js_code))\n",
    "  #--msg logger (END) --#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I_v5z2G5w0Lu"
   },
   "source": [
    "<img src=\"http://lapix.ufsc.br/wp-content/uploads/2022/10/rodape-lapix.png\">\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "VnVY1FTo36KP",
    "_tB-8ebt4q_2"
   ],
   "provenance": [
    {
     "file_id": "1p8k-0HLQV0PsIN3a9k2Z9QDsA1ld2HIr",
     "timestamp": 1669135482647
    }
   ],
   "toc_visible": true
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
